{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f41650",
   "metadata": {},
   "source": [
    "# Covid Tweet Sentiment Classifier\n",
    "\n",
    "Write a description of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977cb6b",
   "metadata": {},
   "source": [
    "## 0. Imports\n",
    "\n",
    "Include discussion about file structure and choice of machine learning frameworks (for us, it will be PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6192eb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports succeeded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import engine\n",
    "#import tweepy\n",
    "\n",
    "print(\"Imports succeeded\")\n",
    "#engine.sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeac00e",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "We need to perform the following steps:\n",
    "- [x] Read the raw data\n",
    "- [X] Scrape the tweets using the IDs\n",
    "- [X] Bin the annotations into either legit information or misinformation\n",
    "\n",
    "Ideally, we create a new dataset .csv and save it. It should have the following headers: \"id\", \"text\", \"annotation1\",\"annotation2,\"class label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd8203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to perform preprocessing again\n",
    "# raw_data = pd.read_csv('../data/miscov19.csv')\n",
    "# raw_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c479a",
   "metadata": {},
   "source": [
    "Next, we select the relevant columns for classification. We keep the ID just in case we want to reference the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e9048ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to perform data preprocessing again\n",
    "# dataset = raw_data[['status_id','text','annotation1', 'annotation2']]\n",
    "# uniq = raw_data[\"annotation1\"].unique()\n",
    "# print(uniq)\n",
    "# uniq1 = raw_data[\"annotation2\"].unique()\n",
    "# print(uniq1)\n",
    "# dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9e9d7",
   "metadata": {},
   "source": [
    "Here, we bin the annotations into legit or illegitimate information in the following way:\n",
    "- Legit information (Label = 1):\n",
    "    - 'true public health response'\n",
    "    - 'calling out or correction'\n",
    "    - 'sarcasm or satire' (not entirely sure if we should say this is legit)\n",
    "    - 'news'\n",
    "    - 'true prevention'\n",
    "    - 'emergency'\n",
    "- Misinformation (Label = 2):\n",
    "    - 'fake cure'\n",
    "    - 'consipiracy'\n",
    "    - 'false fact or prevention'\n",
    "    - 'panic buying'\n",
    "    - 'fake treatment'\n",
    "    - 'false public health response'\n",
    "- Irrelevant to conversation (Label = 3):\n",
    "    - 'irrelevant'\n",
    "    - 'politics' (can technically be legit or misinformation)\n",
    "    - 'ambiguous or hard to classify'\n",
    "    - 'commercial activity or promotion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d025e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def legit(value):\n",
    "    if(value == 'true public health response'\\\n",
    "      or value == 'calling out or correction'\\\n",
    "      or value == 'sarcasm or satire'\\\n",
    "      or value == 'news'\n",
    "      or value == 'true prevention'\n",
    "      or value == 'emergency'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def misinformation(value):\n",
    "    if (value == 'fake cure' \\\n",
    "       or value == 'conspiracy'\\\n",
    "       or value == 'false fact or prevention'\\\n",
    "       or value == 'panic buying'\\\n",
    "       or value == 'fake treatment'\\\n",
    "       or value == 'false public health response'):\n",
    "         return True\n",
    "    return False\n",
    "\n",
    "def irrelevant(value):\n",
    "    if (value == 'irrelevant' \\\n",
    "        or value == 'politics'\\\n",
    "        or value == 'ambiguous or hard to classify'\\\n",
    "        or value == 'commercial activity or promotion'):\n",
    "          return True\n",
    "    return False\n",
    "\n",
    "def bin_annotations(ann):\n",
    "    if irrelevant(ann):\n",
    "        return 100\n",
    "    elif misinformation(ann):\n",
    "        return 10\n",
    "    elif legit(ann):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def map_annotations(ann1, ann2):\n",
    "    num = bin_annotations(ann1) + bin_annotations(ann2)\n",
    "    if num >= 100:\n",
    "        return 3\n",
    "    elif num >= 10:\n",
    "        return 2\n",
    "    elif num >= 1:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c6949af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to redo binning\n",
    "# map_annotations(dataset['annotation1'])\n",
    "# dataset['label'] = dataset.apply(lambda x: map_annotations(x.annotation1, x.annotation2), axis=1)\n",
    "# dataset.to_csv('../data/miscov19_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025e4ad",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "In this section, we will train the model. The model will be defined in the engine.py file along with data loading and associated functions. We will need to develop an API to go between this Jupyter Notebook and the engine. This layer of separation is intentional, as it allows us to have pipeline components which are self contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2eaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90f50c3f",
   "metadata": {},
   "source": [
    "### 2.1 Hyperparameter Optimization \n",
    "\n",
    "Using K-fold validation, we shall attempt to find optimal hyperparameters. The hyperparameter space has yet to be defined, but early stopping is known to be an important aspect of fine tuning BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9a9df",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "In this section, we will evaluate the model based on its performance on a holdout set of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db89fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4167\n",
      "Precision: 0.2500\n",
      "Recall: 0.2000\n",
      "F1: 0.2222\n"
     ]
    }
   ],
   "source": [
    "import model_evaluate\n",
    "\n",
    "#model_evaluate.sanity_check()\n",
    "\n",
    "X_test = [0,0,0,0,0,0,0,0,0,0,0,0] # list-like of test data\n",
    "y_real = [1,2,3,1,2,3,1,2,3,1,2,3] # list-like of real labels on the test data\n",
    "y_pred = [1,2,3,2,1,3,3,2,1,3,1,2] # list-like of predicted labels on the test data\n",
    "\n",
    "acc = model_evaluate.accuracy(y_real, y_pred)\n",
    "pre = model_evaluate.precision(y_real, y_pred)\n",
    "rec = model_evaluate.recall(y_real, y_pred)\n",
    "f1 = model_evaluate.f1(y_real, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\\nPrecision: {pre:.4f}\\nRecall: {rec:.4f}\\nF1: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
